{
 "metadata": {
  "name": "",
  "signature": "sha256:6967705a96acca042c7dd0d6bed235de8f350a809a75d532db280ec1ae0e6e95"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This file is a editted version of https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py\n",
      "# Use Lasagne for digit recognition using  MNIST dataset.\n",
      "import os\n",
      "import numpy as np\n",
      "import sys\n",
      "import time\n",
      "import theano\n",
      "import theano.tensor as T\n",
      "import lasagne\n",
      "#from lasagne.layers.dnn import Conv2DDNNLayer as Conv2DLayer\n",
      "#from lasagne.layers.dnn import MaxPool2DDNNLayer as MaxPool2DLayer\n",
      "from lasagne.layers import Conv2DLayer as Conv2DLayer\n",
      "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayer\n",
      "os.chdir(\"../src/\")\n",
      "\n",
      "def build_cnn(input_var=None, input_label=None):\n",
      "\n",
      "    # Input layer, as usual:\n",
      "    network = lasagne.layers.InputLayer(shape=(None, 1, 28, 28),\n",
      "                                        input_var=input_var)\n",
      "    \n",
      "    # This time we do not apply input dropout, as it tends to work less well\n",
      "    # for convolutional layers.\n",
      "\n",
      "    # Convolutional layer with 32 kernels of size 5x5. Strided and padded\n",
      "    # convolutions are supported as well; see the docstring.\n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=32, filter_size=(5, 5),\n",
      "            #nonlinearity=lasagne.nonlinearities.sigmoid,\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            W=lasagne.init.GlorotUniform())\n",
      "    # Expert note: Lasagne provides alternative convolutional layers that\n",
      "    # override Theano's choice of which implementation to use; for details\n",
      "    # please see http://lasagne.readthedocs.org/en/latest/user/tutorial.html.\n",
      "\n",
      "    # Max-pooling layer of factor 2 in both dimensions:\n",
      "    network = MaxPool2DLayer(network, pool_size=(2, 2))\n",
      "\n",
      "    # Another convolution with 32 5x5 kernels, and another 2x2 pooling:\n",
      "    network = Conv2DLayer(\n",
      "            network, num_filters=32, filter_size=(5, 5),\n",
      "            nonlinearity=lasagne.nonlinearities.rectify,\n",
      "            #nonlinearity=lasagne.nonlinearities.sigmoid\n",
      "            )\n",
      "    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n",
      "\n",
      "    # A fully-connected layer of 256 units with 50% dropout on its inputs:\n",
      "#     network_output = lasagne.layers.DenseLayer(\n",
      "#             network,\n",
      "#             #lasagne.layers.dropout(network, p=.5),\n",
      "#             num_units=256,\n",
      "#             #nonlinearity=lasagne.nonlinearities.sigmoid\n",
      "#             nonlinearity=lasagne.nonlinearities.rectify,\n",
      "#             )\n",
      "    network_output = lasagne.layers.reshape(network, shape = ([0], 512))\n",
      "    \n",
      "#     network = lasagne.layers.ConcatLayer(\n",
      "#             [network_output, labelInput], axis = 1)\n",
      "    # And, finally, the 10-unit output layer with 50% dropout on its inputs:\n",
      "    #network = lasagne.layers.DenseLayer(\n",
      "    #        lasagne.layers.dropout(network, p=.5),\n",
      "    #        num_units=10,\n",
      "    #        nonlinearity=lasagne.nonlinearities.softmax)\n",
      "    \n",
      "    gaussian_output = lasagne.layers.MultiGaussianMixture(network_output, num_components = 5, n_classes = 10, sigma=lasagne.init.Constant(1))\n",
      "    return gaussian_output, network_output\n",
      "\n",
      "\n",
      "\n",
      "def iterate_minibatches(inputs, targets, batchsize, classNum = 10, shuffle=False):\n",
      "    targets = np.array(targets, dtype = np.int32)\n",
      "    assert len(inputs) == len(targets)\n",
      "    if shuffle:\n",
      "        indices = np.arange(len(inputs))\n",
      "        np.random.shuffle(indices)\n",
      "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
      "        if shuffle:\n",
      "            excerpt = indices[start_idx:start_idx + batchsize]\n",
      "        else:\n",
      "            excerpt = slice(start_idx, start_idx + batchsize)\n",
      "        binary_targets = np.zeros((batchsize, classNum))\n",
      "        binary_targets[np.arange(batchsize),targets[excerpt]] = 1\n",
      "        yield inputs[excerpt], np.array(binary_targets,dtype = np.float32)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from dataPreparation import load_data\n",
      "# Load the dataset\n",
      "print(\"Loading data...\")\n",
      "\n",
      "X_train, y_train, X_test, y_test = load_data(\"/X_train.npy\", \"/Y_train.npy\", \"/X_test.npy\", \"/Y_test.npy\")\n",
      "X_train = np.array(X_train, dtype = np.float32)\n",
      "y_train = np.array(y_train, dtype = np.float32)\n",
      "X_test = np.array(X_test, dtype = np.float32)\n",
      "y_test = np.array(y_test, dtype = np.float32)\n",
      "#X_train, y_train, X_test, y_test = load_data(\"/cluttered_train_x.npy\", \"/cluttered_train_y.npy\", \"/cluttered_test_x.npy\", \"/cluttered_test_y.npy\", dataset = \"MNIST_CLUTTER\")\n",
      "\n",
      "# Prepare Theano variables for inputs and targets\n",
      "input_var = T.ftensor4('inputs')\n",
      "target_var = T.fmatrix('targets')\n",
      "\n",
      "# Create neural network model (depending on first command line parameter)\n",
      "\n",
      "network, fc = build_cnn(input_var, target_var)\n",
      "\n",
      "# Create a loss expression for training, i.e., a scalar objective we want\n",
      "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
      "llh_output = lasagne.layers.get_output(network)\n",
      "loss = lasagne.objectives.multi_negative_llh(llh_output, target_var)\n",
      "fc_output = lasagne.layers.get_output(fc)\n",
      "loss_mean = loss.mean() - 470\n",
      "# We could add some weight decay as well here, see lasagne.regularization.\n",
      "\n",
      "# Create update expressions for training, i.e., how to modify the\n",
      "# parameters at each training step. Here, we'll use Stochastic Gradient\n",
      "# Descent (SGD) with Nesterov momentum, but Lasagne offers plenty more.\n",
      "params = lasagne.layers.get_all_params(network, trainable=True)\n",
      "print(params)\n",
      "print(\"model built\")\n",
      "#updates = lasagne.updates.nesterov_momentum(\n",
      "#        loss, params, learning_rate=0.0001, momentum=0.9)\n",
      "gparams = T.grad(loss_mean, params)\n",
      "\n",
      "#0.000001\n",
      "# updates = [(param, param - 0.0000001 * gparam)\n",
      "#     for param, gparam in zip(params[:4], gparams[:4])] + [(params[4], params[4] - 0.01 * gparams[4])]\n",
      "\n",
      "updates = [(param, param - 0.00000001 * gparam)\n",
      "    for param, gparam in zip(params[:4], gparams[:4])] + [(params[4], params[4] - 0.001 * gparams[4])]\n",
      "\n",
      "# Create a loss expression for validation/testing. The crucial difference\n",
      "# here is that we do a deterministic forward pass through the network,\n",
      "# disabling dropout layers.\n",
      "test_llh_output = lasagne.layers.get_output(network, deterministic=True)\n",
      "test_loss = lasagne.objectives.multi_negative_llh(test_llh_output, target_var)\n",
      "test_loss_mean = test_loss.mean() - 470\n",
      "# As a bonus, also create an expression for the classification accuracy:\n",
      "\n",
      "# Compile a function performing a training step on a mini-batch (by giving\n",
      "# the updates dictionary) and returning the corresponding training loss:\n",
      "# + [update_param for update_param in gparams]\n",
      "train_fn = theano.function([input_var, target_var], [loss_mean, llh_output, fc_output,], updates=updates)\n",
      "\n",
      "# Compile a second function computing the validation loss and accuracy:\n",
      "val_fn = theano.function([input_var, target_var], [test_loss_mean, llh_output])\n",
      "\n",
      "# Finally, launch the training loop.\n",
      "print(\"Starting training...\")\n",
      "# We iterate over epochs:\n",
      "num_epochs = 20000\n",
      "for epoch in range(num_epochs):\n",
      "    # In each epoch, we do a full pass over the training data:\n",
      "    train_err = 0\n",
      "    train_batches = 0\n",
      "    start_time = time.time()\n",
      "    batchIndex = 0\n",
      "    for batch in iterate_minibatches(X_train, y_train, 50, 10, shuffle=True):\n",
      "        inputs, targets = batch\n",
      "        #current_loss_mean, current_loss, fc_output = train_fn(inputs, targets)\n",
      "        current_result = train_fn(inputs, targets)\n",
      "\n",
      "#         if batchIndex % 2000 == 0:\n",
      "#             print(current_result)\n",
      "        batchIndex = batchIndex + 1\n",
      "        train_err += current_result[0]\n",
      "        train_batches += 1\n",
      "        break\n",
      "\n",
      "\n",
      "    if epoch % 1000 == 0: \n",
      "        \n",
      "            # Then we print the results for this epoch:\n",
      "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
      "            epoch + 1, num_epochs, time.time() - start_time))\n",
      "        print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
      "        #print(lasagne.layers.get_all_param_values(network)[-3])\n",
      "        print(\"--\")\n",
      "    \n",
      "        # After training, we compute and print the test error:\n",
      "        test_err = 0\n",
      "        test_batches = 0\n",
      "        accurate = 0\n",
      "        for batch in iterate_minibatches(X_test, y_test, 50, 10, shuffle=False):\n",
      "            inputs, targets = batch\n",
      "            valuationResult = val_fn(inputs, targets)\n",
      "            err = valuationResult[0]\n",
      "            test_err += err\n",
      "            test_batches += 1\n",
      "            accurate += np.sum(np.argmax(targets, axis = 1) == np.argmax(valuationResult[1], axis = 1))\n",
      "        print(\"Final results:\")\n",
      "        print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
      "        print(\"Test Accuracy: \", accurate / 10000.0)\n",
      "\n",
      "\n",
      "        # Optionally, you could now dump the network weights to a file like this:\n",
      "        # np.savez('model.npz', *lasagne.layers.get_all_param_values(network))\n",
      "        #\n",
      "        # And load them again later on like this:\n",
      "        # with np.load('model.npz') as f:\n",
      "        #     param_values = [f['arr_%d' % i] for i in range(len(f.files))]\n",
      "        # lasagne.layers.set_all_param_values(network, param_values)\n",
      "# weightsOfParams = lasagne.layers.get_all_param_values(network)\n",
      "# #np.save(\"../data/mnist_clutter_CNN_params_sigmoid.npy\", weightsOfParams)\n",
      "# #np.save(\"../data/mnist_CNN_params_sigmoid.npy\", weightsOfParams)\n",
      "# np.save(\"../data/mnist_CNN_gaussian.npy\", weightsOfParams)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Loading data...\n",
        "[W, b, W, b, Means]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "model built\n",
        "Starting training..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Epoch 1 of 20000 took 0.055s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t261.077026\n",
        "--\n",
        "Final results:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  test loss:\t\t\t273.996702\n",
        "('Test Accuracy: ', 0.11700000000000001)\n",
        "Epoch 1001 of 20000 took 0.028s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  training loss:\t\t175.077942\n",
        "--\n",
        "Final results:"
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "accurate = 0\n",
      "for batch in iterate_minibatches(X_test, y_test, 50, 10, shuffle=False):\n",
      "    inputs, targets = batch\n",
      "    valuationResult = val_fn(inputs, targets)\n",
      "    accurate += np.sum(np.argmax(targets, axis = 1) == np.argmax(valuationResult[1], axis = 1))\n",
      "print(accurate / 10000.0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.8398\n"
       ]
      }
     ],
     "prompt_number": 52
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}